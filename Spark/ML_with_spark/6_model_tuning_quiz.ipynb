{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning Quiz\n",
    "Use this Jupyter notebook to find the answer to the quiz in the previous section. There is an answer key in the next part of the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import RegexTokenizer,CountVectorizer,IDF,StringIndexer\n",
    "# TODOS: \n",
    "# 1) import any other libraries you might need\n",
    "# 2) run the cells below to read dataset\n",
    "# 3) follow the steps below to find the answer to the quiz question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Creating Features\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_overflow_data = 'Train_onetag_small.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Body: string, Id: bigint, Tags: string, Title: string, oneTag: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.json(stack_overflow_data)\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Body: string (nullable = true)\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Tags: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- oneTag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|             oneTag|\n",
      "+-------------------+\n",
      "|                 qt|\n",
      "|           keyboard|\n",
      "|                cpu|\n",
      "|      documentation|\n",
      "|algebra-precalculus|\n",
      "|          windows-7|\n",
      "|   image-processing|\n",
      "|            magento|\n",
      "|             iphone|\n",
      "|           geometry|\n",
      "|    database-design|\n",
      "|windows-server-2008|\n",
      "|               unix|\n",
      "|     zend-framework|\n",
      "|      asp.net-mvc-2|\n",
      "|                ftp|\n",
      "|              xcode|\n",
      "|             debian|\n",
      "|              azure|\n",
      "|            android|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('t1')\n",
    "spark.sql('''\n",
    "select distinct oneTag\n",
    "from t1''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "What is the accuracy of the best model trained with the parameter grid described above (and keeping all other parameters at their default value computed on the 10% untouched data?\n",
    "\n",
    "### Step 1. Train Test Split\n",
    "As a first step break your data set into 90% of training data and set aside 10%. Set random seed to `42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write your code for this step\n",
    "train, test=df.randomSplit([.9,.1],42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write your code for this step\n",
    "tokenzier=RegexTokenizer(inputCol='Body',outputCol='words',pattern='\\\\W')\n",
    "cv=CountVectorizer(inputCol='words',outputCol='TF')\n",
    "idf=IDF(inputCol='TF',outputCol='features')\n",
    "indexer=StringIndexer(inputCol='oneTag',outputCol='label')\n",
    "lrmodel=LogisticRegression(featuresCol='features',labelCol='label')\n",
    "pipline=Pipeline(stages=[tokenzier,cv,idf,indexer,lrmodel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Tune Model\n",
    "On the first 90% of the data let's find the most accurate logistic regression model using 3-fold cross-validation with the following parameter grid:\n",
    "\n",
    "- CountVectorizer vocabulary size: `[1000, 5000]`\n",
    "- LogisticRegression regularization parameter: `[0.0, 0.1]`\n",
    "- LogisticRegression max Iteration number: `[10]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossValidator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "param=ParamGridBuilder() \\\n",
    "    .addGrid(cv.vocabSize,[1000,5000]) \\\n",
    "    .addGrid(lrmodel.maxIter,[10]) \\\n",
    "    .addGrid(lrmodel.regParam,[0.0,0.1]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up crossvalidation\n",
    "crossval=CrossValidator(estimator=pipline,\n",
    "                       estimatorParamMaps=param,\n",
    "                       evaluator=MulticlassClassificationEvaluator(),\n",
    "                       numFolds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compute Accuracy of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write your code for this step\n",
    "model=crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre=model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+\n",
      "|avg(CASE WHEN (prediction = label) THEN 1 ELSE 0 END)|\n",
      "+-----------------------------------------------------+\n",
      "|                                    0.392378263937897|\n",
      "+-----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre.createOrReplaceTempView('t2')\n",
    "spark.sql('''\n",
    "select avg(case when prediction =label then 1\n",
    "            else 0\n",
    "            end)\n",
    "from t2''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test=pre.select('prediction','label').rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=MulticlassMetrics(predictionAndLabels=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.392378263937897"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.fMeasure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5136138613861386"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall(label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39040451552210725"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision(label=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
